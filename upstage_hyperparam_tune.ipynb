{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "10i3IaOsM1mGfTINj189II6UmdjCiWyNo",
      "authorship_tag": "ABX9TyNF7Dr5JWk2YZybeHiJEAV+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Cheeyoung-Yoon/4jua/blob/main/upstage_hyperparam_tune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "H6AED31dNG4D"
      },
      "outputs": [],
      "source": [
        "import pickle as pickle\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "\n",
        "class RE_Dataset(torch.utils.data.Dataset):\n",
        "  \"\"\" Dataset 구성을 위한 class.\"\"\"\n",
        "  def __init__(self, pair_dataset, labels):\n",
        "    self.pair_dataset = pair_dataset\n",
        "    self.labels = labels\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    item = {key: val[idx].clone().detach() for key, val in self.pair_dataset.items()}\n",
        "    item['labels'] = torch.tensor(self.labels[idx])\n",
        "    return item\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.labels)\n",
        "\n",
        "def preprocessing_dataset(dataset):\n",
        "  \"\"\" 처음 불러온 csv 파일을 원하는 형태의 DataFrame으로 변경 시켜줍니다.\"\"\"\n",
        "  subject_entity = []\n",
        "  object_entity = []\n",
        "  for i,j in zip(dataset['subject_entity'], dataset['object_entity']):\n",
        "    i = i[1:-1].split(',')[0].split(':')[1]\n",
        "    j = j[1:-1].split(',')[0].split(':')[1]\n",
        "\n",
        "    subject_entity.append(i)\n",
        "    object_entity.append(j)\n",
        "  out_dataset = pd.DataFrame({'id':dataset['id'], 'sentence':dataset['sentence'],'subject_entity':subject_entity,'object_entity':object_entity,'label':dataset['label'],})\n",
        "  return out_dataset\n",
        "\n",
        "def load_data(dataset_dir):\n",
        "  \"\"\" csv 파일을 경로에 맡게 불러 옵니다. \"\"\"\n",
        "  pd_dataset = pd.read_csv(dataset_dir)\n",
        "  dataset = preprocessing_dataset(pd_dataset)\n",
        "\n",
        "  return dataset\n",
        "\n",
        "\n",
        "def tokenized_dataset(dataset, tokenizer, use_type_markers=True, use_unk=True, max_len=256):\n",
        "    \"\"\"\n",
        "    dataset: pandas.DataFrame with columns:\n",
        "      - sentence\n",
        "      - subject_entity, object_entity  (dict-like str: {'word':..., 'type':...})\n",
        "    \"\"\"\n",
        "    import ast\n",
        "\n",
        "    def parse_ent(e):\n",
        "        if isinstance(e, str):\n",
        "            try:\n",
        "                e = ast.literal_eval(e)\n",
        "            except:\n",
        "                return None, None\n",
        "        if isinstance(e, dict):\n",
        "            return e.get(\"word\"), e.get(\"type\")\n",
        "        return None, None\n",
        "\n",
        "    enc_inputs, enc_texts = [], []\n",
        "\n",
        "    for s_ent, o_ent, sent in zip(dataset['subject_entity'], dataset['object_entity'], dataset['sentence']):\n",
        "        s_word, s_type = parse_ent(s_ent)\n",
        "        o_word, o_type = parse_ent(o_ent)\n",
        "\n",
        "        # 단어가 누락된 경우 안전장치\n",
        "        s_word = s_word if s_word else \"<SUBJ>\"\n",
        "        o_word = o_word if o_word else \"<OBJ>\"\n",
        "\n",
        "        if use_type_markers:\n",
        "            if not s_type and use_unk: s_type = \"UNK\"\n",
        "            if not o_type and use_unk: o_type = \"UNK\"\n",
        "\n",
        "            if s_type and o_type:\n",
        "                e_span = f\"[E1-{s_type}]{s_word}[/E1] [E2-{o_type}]{o_word}[/E2]\"\n",
        "            else:\n",
        "                # 타입을 전혀 모르면 타입 없는 일반 마커 사용\n",
        "                e_span = f\"[E1]{s_word}[/E1] [E2]{o_word}[/E2]\"\n",
        "        else:\n",
        "            # 타입 마커 비활성화: 일반 마커만\n",
        "            e_span = f\"[E1]{s_word}[/E1] [E2]{o_word}[/E2]\"\n",
        "\n",
        "        enc_inputs.append(e_span)\n",
        "        enc_texts.append(sent)\n",
        "\n",
        "    # 필요 시 특수 토큰 등록 (한 번만 실행)\n",
        "    # 타입 마커/일반 마커/종료 마커 + UNK\n",
        "    special_tokens = {\"additional_special_tokens\": [\n",
        "        \"[E1]\",\"[/E1]\",\"[E2]\",\"[/E2]\",\n",
        "        \"[E1-PER]\",\"[E2-PER]\",\"[E1-ORG]\",\"[E2-ORG]\",\n",
        "        \"[E1-LOC]\",\"[E2-LOC]\",\"[E1-UNK]\",\"[E2-UNK]\"\n",
        "    ]}\n",
        "    num_added = tokenizer.add_special_tokens(special_tokens)\n",
        "    # model.resize_token_embeddings(len(tokenizer))  # 모델 로드 후 1회 실행\n",
        "\n",
        "    return tokenizer(\n",
        "        enc_inputs,\n",
        "        enc_texts,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=max_len,\n",
        "        add_special_tokens=True,\n",
        "    )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# trainer_plus.py\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import Trainer\n",
        "\n",
        "def build_llrd_param_groups(model, base_lr=2e-5, lr_decay=0.95, wd=0.01):\n",
        "    \"\"\"\n",
        "    RoBERTa 계열 기준: embeddings -> encoder.layer.0..11 -> pooler, classifier\n",
        "    \"\"\"\n",
        "    groups = []\n",
        "\n",
        "    def add(names, lr, wd):\n",
        "        params = []\n",
        "        for n, p in model.named_parameters():\n",
        "            if any(k in n for k in names) and p.requires_grad:\n",
        "                params.append(p)\n",
        "        if params:\n",
        "            groups.append({\"params\": params, \"lr\": lr, \"weight_decay\": wd})\n",
        "\n",
        "    # embeddings (가장 낮은 lr)\n",
        "    add([\"embeddings\"], base_lr * (lr_decay ** 12), wd)\n",
        "    # 12개 레이어\n",
        "    for i in range(12):\n",
        "        add([f\"encoder.layer.{i}\"], base_lr * (lr_decay ** (11 - i)), wd)\n",
        "    # pooler + classifier (가장 높은 lr)\n",
        "    add([\"pooler\", \"classifier\"], base_lr, wd)\n",
        "    return groups\n",
        "\n",
        "class TrainerPlus(Trainer):\n",
        "    \"\"\"\n",
        "    - class_weights: Tensor | None\n",
        "    - use_focal: bool\n",
        "    - focal_gamma: float\n",
        "    - rdrop_alpha: float (0이면 off)\n",
        "    - use_llrd: bool\n",
        "    - llrd_decay: float\n",
        "    - optimizer_betas: tuple\n",
        "    - wd: float\n",
        "    \"\"\"\n",
        "    def __init__(self, *args,\n",
        "                 class_weights=None,\n",
        "                 use_focal=False, focal_gamma=2.0,\n",
        "                 rdrop_alpha=0.0,\n",
        "                 use_llrd=False, llrd_decay=0.95,\n",
        "                 optimizer_betas=(0.9, 0.999),\n",
        "                 wd=0.01,\n",
        "                 **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.class_weights = class_weights\n",
        "        self.use_focal = use_focal\n",
        "        self.focal_gamma = focal_gamma\n",
        "        self.rdrop_alpha = rdrop_alpha\n",
        "        self.use_llrd = use_llrd\n",
        "        self.llrd_decay = llrd_decay\n",
        "        self.optimizer_betas = optimizer_betas\n",
        "        self._wd = wd\n",
        "\n",
        "    # --- 손실 ---\n",
        "    def _ce(self, logits, labels):\n",
        "        return F.cross_entropy(logits, labels, weight=self.class_weights)\n",
        "\n",
        "    def _focal(self, logits, labels):\n",
        "        ce = F.cross_entropy(logits, labels, reduction=\"none\", weight=self.class_weights)\n",
        "        p = logits.softmax(dim=-1)[torch.arange(len(labels), device=logits.device), labels]\n",
        "        return ((1 - p) ** self.focal_gamma * ce).mean()\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None, **kwargs):\n",
        "        labels = inputs[\"labels\"]\n",
        "        outputs1 = model(**inputs)\n",
        "        logits1 = outputs1.logits\n",
        "        base = self._focal(logits1, labels) if self.use_focal else self._ce(logits1, labels)\n",
        "\n",
        "        if self.rdrop_alpha > 0.0 and self.model.training:\n",
        "            outputs2 = model(**inputs)\n",
        "            logits2 = outputs2.logits\n",
        "            base2 = self._focal(logits2, labels) if self.use_focal else self._ce(logits2, labels)\n",
        "            base = 0.5 * (base + base2)\n",
        "            p1 = logits1.log_softmax(dim=-1)\n",
        "            p2 = logits2.log_softmax(dim=-1)\n",
        "            kl = F.kl_div(p1, p2.exp(), reduction=\"batchmean\") + F.kl_div(p2, p1.exp(), reduction=\"batchmean\")\n",
        "            loss = base + 0.5 * self.rdrop_alpha * kl\n",
        "            return (loss, outputs1) if return_outputs else loss\n",
        "\n",
        "        return (base, outputs1) if return_outputs else base\n",
        "\n",
        "    # --- 옵티마이저 (LLRD/weight-decay 분리) ---\n",
        "    def create_optimizer(self):\n",
        "        if self.optimizer is not None:\n",
        "            return self.optimizer\n",
        "\n",
        "        lr = self.args.learning_rate\n",
        "        wd = self._wd\n",
        "        betas = self.optimizer_betas\n",
        "\n",
        "        no_decay = [\"bias\", \"LayerNorm.weight\", \"LayerNorm.bias\"]\n",
        "\n",
        "        if self.use_llrd:\n",
        "            # layer-wise lr decay 그룹 + no-decay 분리\n",
        "            base_groups = build_llrd_param_groups(self.model, base_lr=lr, lr_decay=self.llrd_decay, wd=wd)\n",
        "            groups = []\n",
        "            for g in base_groups:\n",
        "                params_decay = []\n",
        "                params_nodecay = []\n",
        "                for p in g[\"params\"]:\n",
        "                    name = None\n",
        "                    # param 이름 찾기\n",
        "                    for n, pp in self.model.named_parameters():\n",
        "                        if pp is p:\n",
        "                            name = n; break\n",
        "                    if name and any(nd in name for nd in no_decay):\n",
        "                        params_nodecay.append(p)\n",
        "                    else:\n",
        "                        params_decay.append(p)\n",
        "                if params_decay:\n",
        "                    groups.append({\"params\": params_decay, \"lr\": g[\"lr\"], \"weight_decay\": wd})\n",
        "                if params_nodecay:\n",
        "                    groups.append({\"params\": params_nodecay, \"lr\": g[\"lr\"], \"weight_decay\": 0.0})\n",
        "            param_groups = groups\n",
        "        else:\n",
        "            # 일반 no-decay 분리\n",
        "            decay = []\n",
        "            nodecay = []\n",
        "            for n, p in self.model.named_parameters():\n",
        "                if not p.requires_grad:\n",
        "                    continue\n",
        "                (nodecay if any(nd in n for nd in no_decay) else decay).append(p)\n",
        "            param_groups = [\n",
        "                {\"params\": decay, \"weight_decay\": wd, \"lr\": lr},\n",
        "                {\"params\": nodecay, \"weight_decay\": 0.0, \"lr\": lr},\n",
        "            ]\n",
        "\n",
        "        self.optimizer = torch.optim.AdamW(param_groups, lr=lr, betas=betas)\n",
        "        return self.optimizer\n"
      ],
      "metadata": {
        "id": "TsW6DpZPNPuc"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_re.py\n",
        "import os\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Optional\n",
        "from transformers import AutoTokenizer, AutoConfig, AutoModelForSequenceClassification, TrainingArguments\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# from trainer_plus import TrainerPlus\n",
        "# 아래 4개는 당신이 \"그대로 유지\"한 모듈에서 import 하세요.\n",
        "# from your_module import RE_Dataset, load_data, tokenized_dataset, DEFAULT_LABEL_LIST\n",
        "\n",
        "DEFAULT_LABEL_LIST = [\n",
        "    'no_relation','org:top_members/employees','org:members','org:product','per:title',\n",
        "    'org:alternate_names','per:employee_of','org:place_of_headquarters','per:product',\n",
        "    'org:number_of_employees/members','per:children','per:place_of_residence',\n",
        "    'per:alternate_names','per:other_family','per:colleagues','per:origin',\n",
        "    'per:siblings','per:spouse','org:founded','org:political/religious_affiliation',\n",
        "    'org:member_of','per:parents','org:dissolved','per:schools_attended',\n",
        "    'per:date_of_death','per:date_of_birth','per:place_of_birth','per:place_of_death',\n",
        "    'org:founded_by','per:religion'\n",
        "]\n",
        "\n",
        "@dataclass\n",
        "class TrainConfig:\n",
        "    model_name: str = \"klue/roberta-base\"\n",
        "    output_dir: str = \"./runs\"\n",
        "    num_train_epochs: int = 5\n",
        "    learning_rate: float = 2e-5\n",
        "    per_device_train_batch_size: int = 16\n",
        "    per_device_eval_batch_size: int = 16\n",
        "    warmup_ratio: float = 0.05\n",
        "    weight_decay: float = 0.01\n",
        "    logging_steps: int = 500\n",
        "    save_steps: int = 500\n",
        "    eval_steps: Optional[int] = 500\n",
        "    save_total_limit: int = 3\n",
        "    load_best_model_at_end: bool = False\n",
        "    seed: int = 42\n",
        "    max_length: int = 256\n",
        "    fp16: bool = True\n",
        "    # 입력 마커\n",
        "    marker_variant: str = \"typed\"  # \"typed\" or \"plain\"\n",
        "    # 규제/스케줄러\n",
        "    label_smoothing: float = 0.1\n",
        "    lr_scheduler_type: str = \"cosine\"  # \"linear\" | \"cosine\"\n",
        "    # 손실/가중치/rdrop\n",
        "    use_class_weight: bool = False\n",
        "    use_focal: bool = False\n",
        "    focal_gamma: float = 2.0\n",
        "    rdrop_alpha: float = 0.0\n",
        "    # LLRD\n",
        "    use_llrd: bool = False\n",
        "    llrd_decay: float = 0.95\n",
        "\n",
        "def train_re(\n",
        "    train_csv: str,\n",
        "    dev_csv: Optional[str],\n",
        "    label_list: List[str] = DEFAULT_LABEL_LIST,\n",
        "    cfg: TrainConfig = TrainConfig(),\n",
        "    save_best_to: str = \"./best_model\",\n",
        "    RE_Dataset=None,\n",
        "    load_data=None,\n",
        "    tokenized_dataset=None,\n",
        "):\n",
        "    torch.manual_seed(cfg.seed); np.random.seed(cfg.seed)\n",
        "\n",
        "    # --- Tokenizer/Model ---\n",
        "    tokenizer = AutoTokenizer.from_pretrained(cfg.model_name, use_fast=True)\n",
        "    special_tokens = [\"[E1]\",\"[/E1]\",\"[E2]\",\"[/E2]\"]\n",
        "    if cfg.marker_variant == \"typed\":\n",
        "        special_tokens += [\"[E1-PER]\",\"[E2-PER]\",\"[E1-ORG]\",\"[E2-ORG]\",\"[E1-LOC]\",\"[E2-LOC]\",\"[E1-UNK]\",\"[E2-UNK]\"]\n",
        "    added = tokenizer.add_special_tokens({\"additional_special_tokens\": special_tokens})\n",
        "    config = AutoConfig.from_pretrained(cfg.model_name, num_labels=len(label_list))\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(cfg.model_name, config=config)\n",
        "    if added > 0:\n",
        "        model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "\n",
        "    # --- Data ---\n",
        "    full_df = load_data(train_csv)\n",
        "    if dev_csv:\n",
        "        train_df = full_df\n",
        "        dev_df = load_data(dev_csv)\n",
        "    else:\n",
        "        train_df, dev_df = train_test_split(\n",
        "            full_df, test_size=0.1, random_state=cfg.seed, stratify=full_df['label']\n",
        "        )\n",
        "\n",
        "    label_map = {v: i for i, v in enumerate(label_list)}\n",
        "    train_y = [label_map[v] for v in train_df['label'].values]\n",
        "    dev_y   = [label_map[v] for v in dev_df['label'].values]\n",
        "\n",
        "    tok_train = tokenized_dataset(train_df, tokenizer, max_len=cfg.max_length)\n",
        "    tok_train.pop(\"token_type_ids\", None)\n",
        "    tok_dev   = tokenized_dataset(dev_df, tokenizer, max_len=cfg.max_length)\n",
        "    tok_dev.pop(\"token_type_ids\", None)\n",
        "\n",
        "    RE_train = RE_Dataset(tok_train, train_y)\n",
        "    RE_dev   = RE_Dataset(tok_dev, dev_y)\n",
        "\n",
        "    # --- Class weights (선택) ---\n",
        "    class_weights = None\n",
        "    if cfg.use_class_weight:\n",
        "        counts = np.bincount(train_y, minlength=len(label_list))\n",
        "        inv = 1.0 / np.clip(counts, 1, None)\n",
        "        weights = inv / inv.mean()\n",
        "        class_weights = torch.tensor(weights, dtype=torch.float32, device=device)\n",
        "\n",
        "    # --- Args ---\n",
        "    evaluation_strategy = \"steps\"\n",
        "    args = TrainingArguments(\n",
        "        output_dir=cfg.output_dir,\n",
        "        save_total_limit=cfg.save_total_limit,\n",
        "        save_steps=cfg.save_steps,\n",
        "        num_train_epochs=cfg.num_train_epochs,\n",
        "        learning_rate=cfg.learning_rate,\n",
        "        per_device_train_batch_size=cfg.per_device_train_batch_size,\n",
        "        per_device_eval_batch_size=cfg.per_device_eval_batch_size,\n",
        "        weight_decay=cfg.weight_decay,\n",
        "        logging_steps=cfg.logging_steps,\n",
        "        logging_strategy=\"steps\",\n",
        "        eval_strategy=evaluation_strategy,\n",
        "        eval_steps=cfg.eval_steps,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"micro f1 score\",\n",
        "        greater_is_better=True,\n",
        "        seed=cfg.seed,\n",
        "        fp16=cfg.fp16 and torch.cuda.is_available(),\n",
        "        remove_unused_columns=False,\n",
        "        dataloader_pin_memory=torch.cuda.is_available(),\n",
        "        report_to=\"none\",\n",
        "        label_smoothing_factor=cfg.label_smoothing,\n",
        "        lr_scheduler_type=cfg.lr_scheduler_type,\n",
        "        warmup_ratio=cfg.warmup_ratio,\n",
        "    )\n",
        "\n",
        "    # --- Metrics ---\n",
        "    import sklearn\n",
        "    from sklearn.metrics import accuracy_score\n",
        "    def micro_f1_wo_no_relation(preds, labels, label_list, no_rel=\"no_relation\"):\n",
        "        no_rel_idx = label_list.index(no_rel)\n",
        "        use_labels = [i for i in range(len(label_list)) if i != no_rel_idx]\n",
        "        return sklearn.metrics.f1_score(labels, preds, average=\"micro\", labels=use_labels) * 100.0\n",
        "    def auprc_all(probs, labels, num_labels):\n",
        "        labels_oh = np.eye(num_labels)[labels]\n",
        "        score = []\n",
        "        for c in range(num_labels):\n",
        "            t = labels_oh[:, c]; p = probs[:, c]\n",
        "            prec, rec, _ = sklearn.metrics.precision_recall_curve(t, p)\n",
        "            score.append(sklearn.metrics.auc(rec, prec))\n",
        "        return float(np.mean(score) * 100.0)\n",
        "    def compute_metrics(eval_pred):\n",
        "        logits, labels = eval_pred\n",
        "        if isinstance(logits, tuple):  # HF 버전 호환\n",
        "            logits = logits[0]\n",
        "        preds = logits.argmax(-1)\n",
        "\n",
        "        # 확률(Softmax) → AUPRC 계산용\n",
        "        probs = np.asarray(torch.softmax(torch.tensor(logits), dim=-1))\n",
        "\n",
        "        micro_f1 = micro_f1_wo_no_relation(preds, labels, label_list)\n",
        "        auprc    = auprc_all(probs, labels, num_labels=logits.shape[1])\n",
        "        acc      = accuracy_score(labels, preds) * 100.0\n",
        "\n",
        "        return {\n",
        "            \"micro f1 score\": micro_f1,\n",
        "            \"auprc\": auprc,\n",
        "            \"accuracy\": acc,\n",
        "        }\n",
        "\n",
        "    # --- Trainer ---\n",
        "    trainer = TrainerPlus(\n",
        "        model=model,\n",
        "        args=args,\n",
        "        train_dataset=RE_train,\n",
        "        eval_dataset=RE_dev,\n",
        "        compute_metrics=compute_metrics,\n",
        "        tokenizer=tokenizer,\n",
        "        # plus options\n",
        "        class_weights=class_weights,\n",
        "        use_focal=cfg.use_focal,\n",
        "        focal_gamma=cfg.focal_gamma,\n",
        "        rdrop_alpha=cfg.rdrop_alpha,\n",
        "        use_llrd=cfg.use_llrd,\n",
        "        llrd_decay=cfg.llrd_decay,\n",
        "        wd=cfg.weight_decay,\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    os.makedirs(save_best_to, exist_ok=True)\n",
        "    trainer.save_model(save_best_to)\n",
        "    if trainer.tokenizer: trainer.tokenizer.save_pretrained(save_best_to)\n",
        "    return trainer\n"
      ],
      "metadata": {
        "id": "-2RT5IUUNqG2"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# grid_plus.py\n",
        "import os, time, gc, shutil, pandas as pd, torch\n",
        "from itertools import product\n",
        "from dataclasses import replace\n",
        "from typing import Optional, List, Dict, Any\n",
        "import inspect\n",
        "import numpy as np\n",
        "\n",
        "# from train_re import TrainConfig, train_re, DEFAULT_LABEL_LIST\n",
        "# 아래 4개는 당신이 유지한 구현을 인자로 넘겨서 사용\n",
        "# from your_module import RE_Dataset, load_data, tokenized_dataset\n",
        "from transformers import TrainerCallback\n",
        "from transformers import EarlyStoppingCallback\n",
        "\n",
        "\n",
        "class ConsoleLogger(TrainerCallback):\n",
        "    def on_train_begin(self, args, state, control, **kwargs):\n",
        "        print(f\"[train] start → output_dir={args.output_dir}, \"\n",
        "              f\"lr={args.learning_rate}, bsz={args.per_device_train_batch_size}, \"\n",
        "              f\"epochs={args.num_train_epochs}, max_len={getattr(args, 'max_length', 'N/A')}\")\n",
        "\n",
        "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
        "        # HF가 주기적으로 호출하는 로깅 지점. loss/learning_rate/epoch/step 등이 들어옴\n",
        "        if not state.is_world_process_zero or not logs:\n",
        "            return\n",
        "        keys = [\"loss\",\"learning_rate\",\"epoch\"]\n",
        "        msg = \" | \".join([f\"{k}={logs[k]:.5f}\" for k in keys if k in logs])\n",
        "        if \"eval_micro f1 score\" in logs:\n",
        "            msg += (f\" | microF1={logs['eval_micro f1 score']:.3f}\"\n",
        "                    f\" | AUPRC={logs.get('eval_auprc', logs.get('auprc', float('nan'))):.3f}\"\n",
        "                    f\" | acc={logs.get('eval_accuracy', logs.get('accuracy', float('nan'))):.3f}\")\n",
        "        print(f\"[step {state.global_step}] {msg}\")\n",
        "\n",
        "    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n",
        "        if state.is_world_process_zero and metrics:\n",
        "            print(\"[eval] \"\n",
        "                  f\"microF1={metrics.get('micro f1 score'):.3f} | \"\n",
        "                  f\"AUPRC={metrics.get('auprc'):.3f} | \"\n",
        "                  f\"acc={metrics.get('accuracy'):.3f}\")\n",
        "\n",
        "    def on_train_end(self, args, state, control, **kwargs):\n",
        "        print(f\"[train] end. best_ckpt={state.best_model_checkpoint}\")\n",
        "\n",
        "\n",
        "def run_grid_plus(\n",
        "    train_csv: str,\n",
        "    dev_csv: Optional[str],\n",
        "    models: List[str],\n",
        "    hp_space: Dict[str, list],\n",
        "    base_out: str = \"./grid_runs_plus\",\n",
        "    seed_list: List[int] = (42,),\n",
        "    label_list: List[str] = None,\n",
        "    RE_Dataset=None, load_data=None, tokenized_dataset=None,\n",
        "    callbacks: Optional[List[TrainerCallback]] = None,   # ← 추가\n",
        ") -> pd.DataFrame:\n",
        "    if label_list is None:\n",
        "        label_list = DEFAULT_LABEL_LIST\n",
        "    os.makedirs(base_out, exist_ok=True)\n",
        "\n",
        "    # 기본 콜백 구성\n",
        "    default_callbacks: List[TrainerCallback] = [\n",
        "        ConsoleLogger(),\n",
        "        EarlyStoppingCallback(early_stopping_patience=5)\n",
        "    ]\n",
        "    use_callbacks = default_callbacks if callbacks is None else (default_callbacks + callbacks)\n",
        "\n",
        "    keys, values = zip(*hp_space.items())\n",
        "    combos = list(product(*values))\n",
        "    total = len(models) * len(seed_list) * len(combos)\n",
        "    results = []\n",
        "    idx = 0\n",
        "\n",
        "    for model_name in models:\n",
        "        for seed in seed_list:\n",
        "            for vals in combos:\n",
        "                idx += 1\n",
        "                opt = dict(zip(keys, vals))\n",
        "                run_name = (\n",
        "                    f\"{model_name.replace('/','_')}\"\n",
        "                    f\"_lr{opt['lr']}_ep{opt['epochs']}_bs{opt['train_bsz']}\"\n",
        "                    f\"_ml{opt['max_len']}_sch{opt['scheduler']}\"\n",
        "                    f\"_ls{opt['label_smoothing']}_mk{opt['marker_variant']}\"\n",
        "                    f\"_cw{int(opt['use_class_weight'])}_fc{int(opt['use_focal'])}\"\n",
        "                    f\"_rd{opt['rdrop_alpha']}_llrd{int(opt['use_llrd'])}_seed{seed}\"\n",
        "                )\n",
        "                out_dir = os.path.join(base_out, run_name)\n",
        "                best_dir = os.path.join(out_dir, \"best\")\n",
        "\n",
        "                if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
        "                gc.collect()\n",
        "\n",
        "                t0 = time.time()\n",
        "                row = {\n",
        "                    \"model\": model_name, \"seed\": seed, **opt,\n",
        "                    \"output_dir\": out_dir, \"best_dir\": best_dir,\n",
        "                    \"micro_f1\": None, \"auprc\": None, \"accuracy\": None,\n",
        "                    \"best_ckpt\": None, \"seconds\": None, \"error\": None,\n",
        "                }\n",
        "                try:\n",
        "                    cfg = TrainConfig(\n",
        "                        model_name=model_name,\n",
        "                        output_dir=out_dir,\n",
        "                        num_train_epochs=opt[\"epochs\"],\n",
        "                        learning_rate=opt[\"lr\"],\n",
        "                        per_device_train_batch_size=opt[\"train_bsz\"],\n",
        "                        per_device_eval_batch_size=opt[\"train_bsz\"],\n",
        "                        warmup_ratio=opt[\"warmup_ratio\"],\n",
        "                        weight_decay=0.01,\n",
        "                        logging_steps=500,\n",
        "                        save_steps=500,\n",
        "                        eval_steps=500,\n",
        "                        save_total_limit=3,\n",
        "                        load_best_model_at_end=True,\n",
        "                        seed=seed,\n",
        "                        max_length=opt[\"max_len\"],\n",
        "                        fp16=torch.cuda.is_available(),\n",
        "                        marker_variant=opt[\"marker_variant\"],\n",
        "                        label_smoothing=opt[\"label_smoothing\"],\n",
        "                        lr_scheduler_type=opt[\"scheduler\"],\n",
        "                        use_class_weight=opt[\"use_class_weight\"],\n",
        "                        use_focal=opt[\"use_focal\"],\n",
        "                        focal_gamma=opt[\"focal_gamma\"],\n",
        "                        rdrop_alpha=opt[\"rdrop_alpha\"],\n",
        "                        use_llrd=opt[\"use_llrd\"],\n",
        "                        llrd_decay=opt[\"llrd_decay\"],\n",
        "                    )\n",
        "\n",
        "                    # 1) train_re가 callbacks를 받으면 그대로 전달\n",
        "                    accepts_callbacks = \"callbacks\" in inspect.signature(train_re).parameters\n",
        "                    if accepts_callbacks:\n",
        "                        trainer = train_re(\n",
        "                            train_csv=train_csv,\n",
        "                            dev_csv=dev_csv,\n",
        "                            label_list=label_list,\n",
        "                            cfg=cfg,\n",
        "                            save_best_to=best_dir,\n",
        "                            RE_Dataset=RE_Dataset,\n",
        "                            load_data=load_data,\n",
        "                            tokenized_dataset=tokenized_dataset,\n",
        "                            callbacks=use_callbacks,   # ← 전달\n",
        "                        )\n",
        "                    else:\n",
        "                        trainer = train_re(\n",
        "                            train_csv=train_csv,\n",
        "                            dev_csv=dev_csv,\n",
        "                            label_list=label_list,\n",
        "                            cfg=cfg,\n",
        "                            save_best_to=best_dir,\n",
        "                            RE_Dataset=RE_Dataset,\n",
        "                            load_data=load_data,\n",
        "                            tokenized_dataset=tokenized_dataset\n",
        "                        )\n",
        "                        # 2) 반환된 Trainer에 add_callback으로 주입\n",
        "                        for cb in use_callbacks:\n",
        "                            trainer.add_callback(cb)\n",
        "\n",
        "                    metrics = trainer.evaluate()\n",
        "                    row[\"micro_f1\"] = metrics.get(\"micro f1 score\")\n",
        "                    row[\"auprc\"]    = metrics.get(\"auprc\")\n",
        "                    row[\"accuracy\"] = metrics.get(\"accuracy\")\n",
        "                    state = getattr(trainer, \"state\", None)\n",
        "                    row[\"best_ckpt\"] = getattr(state, \"best_model_checkpoint\", None)\n",
        "\n",
        "                except Exception as e:\n",
        "                    row[\"error\"] = f\"{type(e).__name__}: {e}\"\n",
        "                finally:\n",
        "                    row[\"seconds\"] = round(time.time() - t0, 2)\n",
        "                    results.append(row)\n",
        "                    if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
        "                    gc.collect()\n",
        "                    shutil.rmtree(out_dir, ignore_errors=True)\n",
        "\n",
        "    df = pd.DataFrame(results).sort_values(\n",
        "        by=[\"micro_f1\",\"auprc\",\"accuracy\"], ascending=False, na_position=\"last\"\n",
        "    ).reset_index(drop=True)\n",
        "    df.to_csv(os.path.join(base_out, \"param_grid_summary.csv\"), index=False, encoding=\"utf-8-sig\")\n",
        "    return df"
      ],
      "metadata": {
        "id": "BelJlWERN07J"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run.py (노트북/스크립트에서)\n",
        "# from grid_plus import run_grid_plus\n",
        "# from train_re import DEFAULT_LABEL_LIST\n",
        "# # 당신이 유지한 구현\n",
        "# from your_module import RE_Dataset, load_data, tokenized_dataset\n",
        "\n",
        "HP_SPACE = {\n",
        "  \"lr\":              [1.5e-5, 2e-5, 2.5e-5],\n",
        "  \"epochs\":          [10],          # 10에서 과적합 체크\n",
        "  \"train_bsz\":       [16, 32],             # 32는 일단 제외\n",
        "  \"max_len\":         [192, 256],\n",
        "  \"scheduler\":       [\"cosine\"],\n",
        "  \"warmup_ratio\":    [0.05, 0.1],      # 워밍업 살짝 확대 실험\n",
        "  \"label_smoothing\": [0.0, 0.1],       # 간단 규제\n",
        "  \"marker_variant\":  [\"typed\"],        # 현재 성능 좋은 쪽 고정\n",
        "  \"use_class_weight\":[False, True],    # 불균형 완화 체크\n",
        "  \"use_focal\":       [False],          # focal은 v3로 미룸\n",
        "  \"focal_gamma\":     [2.0],\n",
        "  \"rdrop_alpha\":     [0.0, 1.0],       # 가벼운 R-Drop\n",
        "  \"use_llrd\":        [False, True],    # LLRD 효과 확인\n",
        "  \"llrd_decay\":      [0.95],\n",
        "}\n",
        "\n",
        "MODELS = [\"klue/roberta-base\"]\n",
        "\n",
        "df = run_grid_plus(\n",
        "  train_csv=\"/content/drive/MyDrive/Colab Notebooks/upstage/dataset/train.csv\",\n",
        "  dev_csv=None,  # dev 없으면 내부에서 0.1 stratify split\n",
        "  models=MODELS,\n",
        "  hp_space=HP_SPACE,\n",
        "  base_out=\"./grid_runs_plus\",\n",
        "  seed_list=[42],\n",
        "  label_list=DEFAULT_LABEL_LIST,\n",
        "  RE_Dataset=RE_Dataset,\n",
        "  load_data=load_data,\n",
        "  tokenized_dataset=tokenized_dataset,\n",
        ")\n",
        "df.head()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 508
        },
        "id": "FEAsx-D7N8i0",
        "outputId": "9e9864fa-d2dc-4622-c080-61571df0d610"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/tmp/ipython-input-3066431769.py:48: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `TrainerPlus.__init__`. Use `processing_class` instead.\n",
            "  super().__init__(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1827' max='1827' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1827/1827 06:23, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Micro f1 score</th>\n",
              "      <th>Auprc</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>2.248200</td>\n",
              "      <td>1.778522</td>\n",
              "      <td>43.689788</td>\n",
              "      <td>24.493090</td>\n",
              "      <td>48.906683</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>1.652800</td>\n",
              "      <td>1.562364</td>\n",
              "      <td>51.931602</td>\n",
              "      <td>30.952617</td>\n",
              "      <td>53.403141</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>1.501800</td>\n",
              "      <td>1.441856</td>\n",
              "      <td>54.840925</td>\n",
              "      <td>32.977617</td>\n",
              "      <td>57.068063</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='203' max='203' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [203/203 00:07]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[step 1827] epoch=1.00000 | microF1=54.841 | AUPRC=32.978 | acc=57.068\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "               model  seed        lr  epochs  train_bsz  max_len scheduler  \\\n",
              "0  klue/roberta-base    42  0.000015       1         16      192    cosine   \n",
              "\n",
              "   warmup_ratio  label_smoothing marker_variant  ...  use_llrd  llrd_decay  \\\n",
              "0          0.05             0.01          typed  ...     False        0.95   \n",
              "\n",
              "                                          output_dir  \\\n",
              "0  ./grid_runs_plus/klue_roberta-base_lr1.5e-05_e...   \n",
              "\n",
              "                                            best_dir  micro_f1  auprc  \\\n",
              "0  ./grid_runs_plus/klue_roberta-base_lr1.5e-05_e...      None   None   \n",
              "\n",
              "  accuracy best_ckpt seconds  \\\n",
              "0     None      None  410.73   \n",
              "\n",
              "                                               error  \n",
              "0  TypeError: unsupported format string passed to...  \n",
              "\n",
              "[1 rows x 24 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e77a61fc-ca56-4970-b525-1f05ccf5eb26\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model</th>\n",
              "      <th>seed</th>\n",
              "      <th>lr</th>\n",
              "      <th>epochs</th>\n",
              "      <th>train_bsz</th>\n",
              "      <th>max_len</th>\n",
              "      <th>scheduler</th>\n",
              "      <th>warmup_ratio</th>\n",
              "      <th>label_smoothing</th>\n",
              "      <th>marker_variant</th>\n",
              "      <th>...</th>\n",
              "      <th>use_llrd</th>\n",
              "      <th>llrd_decay</th>\n",
              "      <th>output_dir</th>\n",
              "      <th>best_dir</th>\n",
              "      <th>micro_f1</th>\n",
              "      <th>auprc</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>best_ckpt</th>\n",
              "      <th>seconds</th>\n",
              "      <th>error</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>klue/roberta-base</td>\n",
              "      <td>42</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>1</td>\n",
              "      <td>16</td>\n",
              "      <td>192</td>\n",
              "      <td>cosine</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.01</td>\n",
              "      <td>typed</td>\n",
              "      <td>...</td>\n",
              "      <td>False</td>\n",
              "      <td>0.95</td>\n",
              "      <td>./grid_runs_plus/klue_roberta-base_lr1.5e-05_e...</td>\n",
              "      <td>./grid_runs_plus/klue_roberta-base_lr1.5e-05_e...</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>410.73</td>\n",
              "      <td>TypeError: unsupported format string passed to...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1 rows × 24 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e77a61fc-ca56-4970-b525-1f05ccf5eb26')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e77a61fc-ca56-4970-b525-1f05ccf5eb26 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e77a61fc-ca56-4970-b525-1f05ccf5eb26');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df"
            }
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['error'].iloc[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "bmvW5BATUOsI",
        "outputId": "5f4e62ba-c739-4bcf-db86-732fc7f6b459"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"TypeError: TrainerPlus.compute_loss() got an unexpected keyword argument 'num_items_in_batch'\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ke-0ZyxnjMGR"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_grid_plus(\n",
        "  train_csv=\"/content/drive/MyDrive/Colab Notebooks/upstage/dataset/train.csv\",\n",
        "  dev_csv=None,  # dev 없으면 내부에서 0.1 stratify split\n",
        "  models=MODELS,\n",
        "  hp_space=HP_SPACE,\n",
        "  base_out=\"./grid_runs_plus\",\n",
        "  seed_list=[42],\n",
        "  label_list=DEFAULT_LABEL_LIST,\n",
        "  RE_Dataset=RE_Dataset,\n",
        "  load_data=load_data,\n",
        "  tokenized_dataset=tokenized_dataset,\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "id": "YhmiLzYIUYdv",
        "outputId": "9c19c84f-3842-4152-ad48-e0683a52f5b7"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/tmp/ipython-input-2472577479.py:48: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `TrainerPlus.__init__`. Use `processing_class` instead.\n",
            "  super().__init__(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "               model  seed        lr  epochs  train_bsz  max_len scheduler  \\\n",
              "0  klue/roberta-base    42  0.000015       1         16      192    cosine   \n",
              "\n",
              "   warmup_ratio  label_smoothing marker_variant  ...  use_llrd  llrd_decay  \\\n",
              "0          0.05             0.01          typed  ...     False        0.95   \n",
              "\n",
              "                                          output_dir  \\\n",
              "0  ./grid_runs_plus/klue_roberta-base_lr1.5e-05_e...   \n",
              "\n",
              "                                            best_dir  micro_f1  auprc  \\\n",
              "0  ./grid_runs_plus/klue_roberta-base_lr1.5e-05_e...      None   None   \n",
              "\n",
              "  accuracy best_ckpt seconds  \\\n",
              "0     None      None   11.97   \n",
              "\n",
              "                                               error  \n",
              "0  TypeError: TrainerPlus.compute_loss() got an u...  \n",
              "\n",
              "[1 rows x 24 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-251d6f2c-c266-422f-a204-8caa4ddd4822\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model</th>\n",
              "      <th>seed</th>\n",
              "      <th>lr</th>\n",
              "      <th>epochs</th>\n",
              "      <th>train_bsz</th>\n",
              "      <th>max_len</th>\n",
              "      <th>scheduler</th>\n",
              "      <th>warmup_ratio</th>\n",
              "      <th>label_smoothing</th>\n",
              "      <th>marker_variant</th>\n",
              "      <th>...</th>\n",
              "      <th>use_llrd</th>\n",
              "      <th>llrd_decay</th>\n",
              "      <th>output_dir</th>\n",
              "      <th>best_dir</th>\n",
              "      <th>micro_f1</th>\n",
              "      <th>auprc</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>best_ckpt</th>\n",
              "      <th>seconds</th>\n",
              "      <th>error</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>klue/roberta-base</td>\n",
              "      <td>42</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>1</td>\n",
              "      <td>16</td>\n",
              "      <td>192</td>\n",
              "      <td>cosine</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.01</td>\n",
              "      <td>typed</td>\n",
              "      <td>...</td>\n",
              "      <td>False</td>\n",
              "      <td>0.95</td>\n",
              "      <td>./grid_runs_plus/klue_roberta-base_lr1.5e-05_e...</td>\n",
              "      <td>./grid_runs_plus/klue_roberta-base_lr1.5e-05_e...</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>11.97</td>\n",
              "      <td>TypeError: TrainerPlus.compute_loss() got an u...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1 rows × 24 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-251d6f2c-c266-422f-a204-8caa4ddd4822')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-251d6f2c-c266-422f-a204-8caa4ddd4822 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-251d6f2c-c266-422f-a204-8caa4ddd4822');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe"
            }
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X-sFGY7jgwZH"
      },
      "execution_count": 69,
      "outputs": []
    }
  ]
}